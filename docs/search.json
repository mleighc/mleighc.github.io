[
  {
    "objectID": "learn.html",
    "href": "learn.html",
    "title": "Learn with Me",
    "section": "",
    "text": "This will soon be the home for my self-paced guides, starter kits, and creative tools, designed to help data professionals build confidence, showcase their work, and grow in public (without the overwhelm).\n\nComing soon:\nüõ†Ô∏è Zero to Quarto in 14 Days\nA beginner-friendly starter kit to help you build and publish your own Quarto portfolio website, step by step, no perfection required.\nI‚Äôll be sharing more soon. In the meantime, feel free to follow me on LinkedIn.\nThanks for being here üíõ\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I‚Äôm Miranda!",
    "section": "",
    "text": "I am a data analyst and systems builder who got into tech after several years in nonprofit fundraising operations, where I spent a lot of time managing messy spreadsheets and trying to make broken processes work. Eventually, I realized what I really wanted was to fix the system itself, not just clean up after it.\nI made the pivot into data analytics a few years ago, and since then, I‚Äôve been focused on building tools that make work smoother, clearer, and less redundant. These days, I work at the University of Michigan‚Äôs Alzheimer‚Äôs Disease Center, where I manage the data infrastructure behind our research systems, connecting tools like REDCap, SharePoint, R, and Power Automate to help data move cleanly between teams and projects.\nWhat sets me apart isn‚Äôt just technical fluency, it‚Äôs the ability to see both the big picture and the system underneath.\nI diagram, debug, and document.\nI build with the end user in mind. And I never stop asking: How could this be easier, clearer, or more scalable?\nOn the side, I run a small business called SMPL Sheets that transforms public directories into clean, CRM-ready datasets, and a print-on-demand project called Fluffy Feeling Co., because automation can be fun, too.\n\n\n Back to top"
  },
  {
    "objectID": "projects/madc_resource_library.html",
    "href": "projects/madc_resource_library.html",
    "title": "Michigan Alzheimer‚Äôs Disease Center Resource Library",
    "section": "",
    "text": "I led the development of an online resource site for the Michigan Alzheimer‚Äôs Disease Center‚Äôs (MADC) Data Core. This site is designed to provide a growing collection of technical resources to help onboard new and current staff, improving their skills for day-to-day operations. I utilized Quarto‚Äôs book infrastructure to compile the resources and successfully deployed the site using Quarto Pub, enabling seamless updates after each commit."
  },
  {
    "objectID": "projects/madc_resource_library.html#project-overview",
    "href": "projects/madc_resource_library.html#project-overview",
    "title": "Michigan Alzheimer‚Äôs Disease Center Resource Library",
    "section": "",
    "text": "I led the development of an online resource site for the Michigan Alzheimer‚Äôs Disease Center‚Äôs (MADC) Data Core. This site is designed to provide a growing collection of technical resources to help onboard new and current staff, improving their skills for day-to-day operations. I utilized Quarto‚Äôs book infrastructure to compile the resources and successfully deployed the site using Quarto Pub, enabling seamless updates after each commit."
  },
  {
    "objectID": "projects/madc_resource_library.html#key-accomplishments",
    "href": "projects/madc_resource_library.html#key-accomplishments",
    "title": "Michigan Alzheimer‚Äôs Disease Center Resource Library",
    "section": "Key Accomplishments",
    "text": "Key Accomplishments\n\nDeveloped and launched an online technical resources site for MADC‚Äôs Data Core staff.\nUtilized Quarto‚Äôs book infrastructure to structure and organize content effectively.\nAutomated site deployment to Quarto Pub after each commit by writing and configuring a YAML file.\nEnsured the site remains up-to-date and easily accessible to staff for continuous learning and skill-building.\n\n View the deployed site\n View the source code"
  },
  {
    "objectID": "projects/dmsc_madc_site.html",
    "href": "projects/dmsc_madc_site.html",
    "title": "Data Management and Statistical Core Sharing Hub",
    "section": "",
    "text": "Currently in development, the Data Management and Statistical Core Sharing Hub is a public-facing website designed to showcase resources from the Data Management and Statistical Core at the Michigan Alzheimer‚Äôs Disease Center (MADC). The goal is to provide accessible tools, guidelines, and documentation to support data management and statistical practices in Alzheimer‚Äôs disease research. Built using Quarto, this site integrates Markdown for content creation with the powerful features of Quarto for publishing, making it a dynamic, reproducible, and user-friendly resource for the research community.\nThe site is hosted on GitHub Pages, which simplifies deployment and allows for continuous updates as the Data Core‚Äôs resources evolve."
  },
  {
    "objectID": "projects/dmsc_madc_site.html#project-overview",
    "href": "projects/dmsc_madc_site.html#project-overview",
    "title": "Data Management and Statistical Core Sharing Hub",
    "section": "",
    "text": "Currently in development, the Data Management and Statistical Core Sharing Hub is a public-facing website designed to showcase resources from the Data Management and Statistical Core at the Michigan Alzheimer‚Äôs Disease Center (MADC). The goal is to provide accessible tools, guidelines, and documentation to support data management and statistical practices in Alzheimer‚Äôs disease research. Built using Quarto, this site integrates Markdown for content creation with the powerful features of Quarto for publishing, making it a dynamic, reproducible, and user-friendly resource for the research community.\nThe site is hosted on GitHub Pages, which simplifies deployment and allows for continuous updates as the Data Core‚Äôs resources evolve."
  },
  {
    "objectID": "projects/dmsc_madc_site.html#key-accomplishments",
    "href": "projects/dmsc_madc_site.html#key-accomplishments",
    "title": "Data Management and Statistical Core Sharing Hub",
    "section": "Key Accomplishments:",
    "text": "Key Accomplishments:\n\nQuarto Setup & Deployment:\n\nChose Quarto as the platform for building and managing the website, allowing for easy integration with GitHub Pages for hosting.\nConfigured the site to deploy directly from the docs folder on the main branch, ensuring that all future updates are automatically reflected online.\n\nContributor Workflow & Documentation:\n\nDeveloped a streamlined process for contributors to fork the repository, clone their copy locally, and submit pull requests.\nDetailed instructions were created for setting up VS Code with the necessary Quarto extensions, editing content locally, previewing the website, and pushing changes back to GitHub.\n\nWeb Content Creation:\n\nCreated and customized the main website using Quarto‚Äôs flexibility to write dynamic content in Markdown and Quarto‚Äôs .qmd files.\nIncluded sections such as setup guides, tutorials, and links to external resources, all designed to make the content easily navigable for both technical and non-technical users.\n\nVersion Control & Maintenance:\n\nImplemented version control with Git, providing a clear workflow for contributing to the site and tracking changes.\nUsed GitHub‚Äôs built-in tools to ensure the site stays updated and that contributors can easily collaborate on ongoing improvements.\n\nPublic Availability & Reproducibility:\n\nEnsured the site is publicly accessible, providing transparency and reproducibility for tools and resources related to data management in Alzheimer‚Äôs research.\nThe setup allows for continuous updates and scalability, with Quarto‚Äôs integration ensuring that future contributions to the website can be managed efficiently.\n\nFuture Expansion:\n\nEstablished a foundation for further growth by setting up the website with the flexibility to add new content, resources, and tools as they are developed by the Data Core team.\n\n\n View the deployed site\n View the source code"
  },
  {
    "objectID": "projects/collab_filtering_survey.html",
    "href": "projects/collab_filtering_survey.html",
    "title": "Collaborative Filtering Methodologies",
    "section": "",
    "text": "Recommender systems play a crucial role in modern e-commerce and social networking platforms, addressing the growing need for personalized and relevant item suggestions. This project, completed as a final project for a Data Mining course at the University of Michigan School of Information, explores collaborative filtering, a fundamental category of recommendation algorithms, with a focus on its methodologies and real-world applications in book recommendation systems. Using the Book-Crossing dataset, this study evaluates the performance of various collaborative filtering algorithms to predict user-item ratings and explores book recommendation systems used by platforms like Goodreads, Likewise, and WhatShouldIReadNext?.\nThe study also leverages Python libraries such as Surprise and Recmetrics to build, evaluate, and compare recommender models while examining their practical implications. Key challenges addressed include data sparsity, the ‚Äúgray sheep problem‚Äù (difficulty in serving users with niche preferences), and computational limitations with large datasets. This work provides insights into algorithmic performance and the nuanced trade-offs of collaborative filtering in real-world applications."
  },
  {
    "objectID": "projects/collab_filtering_survey.html#project-overview",
    "href": "projects/collab_filtering_survey.html#project-overview",
    "title": "Collaborative Filtering Methodologies",
    "section": "",
    "text": "Recommender systems play a crucial role in modern e-commerce and social networking platforms, addressing the growing need for personalized and relevant item suggestions. This project, completed as a final project for a Data Mining course at the University of Michigan School of Information, explores collaborative filtering, a fundamental category of recommendation algorithms, with a focus on its methodologies and real-world applications in book recommendation systems. Using the Book-Crossing dataset, this study evaluates the performance of various collaborative filtering algorithms to predict user-item ratings and explores book recommendation systems used by platforms like Goodreads, Likewise, and WhatShouldIReadNext?.\nThe study also leverages Python libraries such as Surprise and Recmetrics to build, evaluate, and compare recommender models while examining their practical implications. Key challenges addressed include data sparsity, the ‚Äúgray sheep problem‚Äù (difficulty in serving users with niche preferences), and computational limitations with large datasets. This work provides insights into algorithmic performance and the nuanced trade-offs of collaborative filtering in real-world applications."
  },
  {
    "objectID": "projects/collab_filtering_survey.html#key-accomplishments",
    "href": "projects/collab_filtering_survey.html#key-accomplishments",
    "title": "Collaborative Filtering Methodologies",
    "section": "Key Accomplishments",
    "text": "Key Accomplishments\n\nDataset Utilization:\n\nProcessed the Book-Crossing dataset containing over 1.1 million user ratings for 271,379 books.\n\nAddressed data sparsity by removing implicit ratings (62% of total), resulting in a more interpretable left-skewed distribution of ratings with a mean score of 7.6.\n\nAlgorithm Evaluation:\n\nExperimented with six collaborative filtering algorithms using the Surprise library: Baseline Only, K-Nearest Neighbors (KNN), and Matrix Factorization models.\n\nAdapted to computational constraints by sampling datasets and iteratively evaluating performance on progressively larger training sets.\n\nTool Adoption and Documentation:\n\nExplored Surprise, a Python library for implementing prediction algorithms and evaluation metrics, and Recmetrics, a toolkit for recommender system diagnostics.\n\nHighlighted key functionalities of both tools, including prediction accuracy and bias diagnostics.\n\nInsights on Collaborative Filtering:\n\nDiscussed challenges such as data sparsity, lack of personalization for niche users, and the potential for biased or manipulated ratings.\n\nHighlighted trade-offs between algorithm simplicity and personalization precision.\n\nReal-World Applications:\n\nAnalyzed book recommendation platforms (Goodreads, Likewise, and WhatShouldIReadNext?) to contextualize findings.\n\nConnected experimental results to practical use cases in improving user engagement through personalized recommendations.\n\nImpactful Visualizations:\n\nGenerated insightful visualizations of the dataset, including the distribution of ratings before and after preprocessing, illustrating the impact of removing implicit data.\n\nEducational Focus:\n\nBridged theory and practice by examining the documentation of recommender system libraries and applying them to a large-scale real-world dataset.\n\nDeveloped an accessible survey of collaborative filtering techniques for practitioners and researchers.\n\n\n View the final report"
  },
  {
    "objectID": "projects/smpl_sheets.html",
    "href": "projects/smpl_sheets.html",
    "title": "Web Scraping for SMPL Sheets, LLC",
    "section": "",
    "text": "SMPL SHEETS was born out of necessity. While exploring methods for relationship management, we found that although public data‚Äîlike Chamber of Commerce directories‚Äîwas highly accessible visually, it wasn‚Äôt formatted for efficient use. Managing large uploads of information to a CRM required time-consuming manual effort.\nWith backgrounds in entrepreneurship and data analysis, we approached the problem with a solutions-oriented mindset. The more we worked through this challenge, the more we saw an opportunity to scale our process and help others. We realized that many businesses could benefit from pre-cleaned, formatted, and CRM-friendly data, saving them countless hours and enabling them to focus on what matters most: building meaningful connections.\nWe‚Äôre starting with Chamber of Commerce data because it‚Äôs relevant to our own relationship management needs, but we‚Äôre just getting started. In the future, we envision expanding into other categories of public data‚Äîacross various sectors‚Äîmaking this valuable information accessible, usable, and impactful.\nAt SMPL SHEETS, we‚Äôre turning complex data into simple solutions, empowering businesses to work smarter, not harder."
  },
  {
    "objectID": "projects/smpl_sheets.html#project-overview",
    "href": "projects/smpl_sheets.html#project-overview",
    "title": "Web Scraping for SMPL Sheets, LLC",
    "section": "",
    "text": "SMPL SHEETS was born out of necessity. While exploring methods for relationship management, we found that although public data‚Äîlike Chamber of Commerce directories‚Äîwas highly accessible visually, it wasn‚Äôt formatted for efficient use. Managing large uploads of information to a CRM required time-consuming manual effort.\nWith backgrounds in entrepreneurship and data analysis, we approached the problem with a solutions-oriented mindset. The more we worked through this challenge, the more we saw an opportunity to scale our process and help others. We realized that many businesses could benefit from pre-cleaned, formatted, and CRM-friendly data, saving them countless hours and enabling them to focus on what matters most: building meaningful connections.\nWe‚Äôre starting with Chamber of Commerce data because it‚Äôs relevant to our own relationship management needs, but we‚Äôre just getting started. In the future, we envision expanding into other categories of public data‚Äîacross various sectors‚Äîmaking this valuable information accessible, usable, and impactful.\nAt SMPL SHEETS, we‚Äôre turning complex data into simple solutions, empowering businesses to work smarter, not harder."
  },
  {
    "objectID": "projects/smpl_sheets.html#key-accomplishments",
    "href": "projects/smpl_sheets.html#key-accomplishments",
    "title": "Web Scraping for SMPL Sheets, LLC",
    "section": "Key Accomplishments",
    "text": "Key Accomplishments\n\nDeveloped a solution to clean and reformat publicly available data, starting with Chamber of Commerce directories.\nAutomated the process of data scraping using Python and BeautifulSoup to extract public data efficiently.\nStreamlined the process of transforming raw data into CRM-friendly formats, saving businesses time on manual data entry.\nBuilt a scalable business model that allows for expansion into other types of public data, offering CRM-friendly solutions across various sectors.\nEstablished a foundation for future growth, with a focus on delivering usable, actionable data that enables better relationship management.\n\n View the deployed site"
  },
  {
    "objectID": "projects/womens_web.html",
    "href": "projects/womens_web.html",
    "title": "Women‚Äôs Web Design Project",
    "section": "",
    "text": "This project was part of an elective course in my Master‚Äôs in Data Analytics at the University of Michigan School of Information. The goal was to improve the structure and design of a basic HTML web page by incorporating JavaScript and CSS. The main focus was on creating an accessible and responsive web page.\nStarting with a pre-written content structure in HTML, I applied CSS for design and layout enhancements and used JavaScript to add interactivity and functionality. The project emphasized best practices for web accessibility to ensure the website was usable by people with various disabilities."
  },
  {
    "objectID": "projects/womens_web.html#project-overview",
    "href": "projects/womens_web.html#project-overview",
    "title": "Women‚Äôs Web Design Project",
    "section": "",
    "text": "This project was part of an elective course in my Master‚Äôs in Data Analytics at the University of Michigan School of Information. The goal was to improve the structure and design of a basic HTML web page by incorporating JavaScript and CSS. The main focus was on creating an accessible and responsive web page.\nStarting with a pre-written content structure in HTML, I applied CSS for design and layout enhancements and used JavaScript to add interactivity and functionality. The project emphasized best practices for web accessibility to ensure the website was usable by people with various disabilities."
  },
  {
    "objectID": "projects/womens_web.html#key-accomplishments",
    "href": "projects/womens_web.html#key-accomplishments",
    "title": "Women‚Äôs Web Design Project",
    "section": "Key Accomplishments",
    "text": "Key Accomplishments\n\nModified basic HTML to improve structure and accessibility, ensuring compatibility with assistive technologies.\nApplied CSS to enhance the visual design, making the web page aesthetically pleasing and user-friendly.\nIntegrated JavaScript to introduce interactivity and improve user engagement.\nImplemented responsive design techniques to ensure the website functions well across different devices and screen sizes.\nFocused on accessibility best practices, ensuring compliance with WCAG (Web Content Accessibility Guidelines).\n\n View the deployed site\n View the source code"
  },
  {
    "objectID": "projects/mastery_capstone_data_analysis.html",
    "href": "projects/mastery_capstone_data_analysis.html",
    "title": "Data Analysis Capstone: Is Education the Key to Success?",
    "section": "",
    "text": "Project Overview\nCompleted in the final capstone course for the Master of Science in Information, this project explores the relationship between education, demographics, and earnings, focusing on addressing questions about the stagnation of wages in the U.S. despite rising productivity. Initially aiming to investigate productivity growth, the project pivoted to examining the impact of education and demographics on earnings after realizing the complexity of the economics involved. The team built a public-facing dashboard to convey findings in an accessible and understandable way for the general public. The dashboard focuses on comparing wages based on education, gender, and race, while also exploring the potential impact of advanced degrees on underemployment.\n\n\nKey Accomplishments\n\nData Collection and Preprocessing:\n\nCollected data from the U.S. Bureau of Labor Statistics‚Äô Current Population Survey (CPS), which contains responses from over 270,000 individuals annually.\nFiltered the data to focus on full-time workers aged 18 and older, adjusting earnings for inflation to 2022 dollars for consistency across years.\n\nDashboard Development:\n\nDeveloped an interactive dashboard using Streamlit, allowing users to explore how education and demographic factors affect wages over time.\nCreated visualizations in Altair to represent wage disparities across gender, race, and education levels.\n\nFocused Analysis:\n\nConducted an analysis of the gender wage gap and its relationship to educational attainment, highlighting persistent disparities in earnings between men and women, particularly for women of color.\nExamined the earning potential of different education levels, demonstrating how higher education can influence wage growth.\n\nEducational Impact:\n\nProvided insights on how educational attainment can reduce the chances of underemployment, and how union membership interacts with education to affect earnings.\n\nTarget Audience and Accessibility:\n\nDesigned the dashboard to cater to young professionals and students, particularly those considering further education, providing accessible data and visualizations to inform career and educational decisions.\n\n\n View the deployed site\n View the source code\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/data_mgmt_trends.html",
    "href": "blogs/data_mgmt_trends.html",
    "title": "üìå 9 Trends Shaping the Future of Data Management in 2025",
    "section": "",
    "text": "I saved this post to get a broader view of where data management is heading, especially as more teams shift away from patchwork tools and toward cohesive, platform-style systems. Now that I‚Äôm deeper into building data infrastructure myself, it felt like the right time to circle back.\nüìå 6 Data Management Trends to Watch in 2025"
  },
  {
    "objectID": "blogs/data_mgmt_trends.html#reflections",
    "href": "blogs/data_mgmt_trends.html#reflections",
    "title": "üìå 9 Trends Shaping the Future of Data Management in 2025",
    "section": "Reflections",
    "text": "Reflections\nWhat stood out to me wasn‚Äôt just the trends themselves, but how much of the framing echoed ideas I‚Äôve been trying to bake into my own work, even if I wouldn‚Äôt have used the same language at the time.\nA few things that stuck:\n\nData observability is getting more sophisticated: it‚Äôs not just about detecting issues, but being able to trace them back to their source with confidence. That resonates with the kind of audit trails and validation steps I‚Äôve been trying to build into our systems.\nPlatform thinking is on the rise: teams are moving away from isolated dashboards or tools and toward end-to-end coordination. That aligns with how I‚Äôve been approaching our internal research systems: not as one-off solutions, but as interconnected parts of a broader flow.\nThe shift in data governance conversations: from rigid gatekeeping to embedded structure that supports collaboration. That one felt particularly relevant. We‚Äôre still figuring this out in practice, but I like the reframing: governance not as a blocker, but as scaffolding.\n\nEven though the article is targeted at large-scale teams and enterprise systems, there‚Äôs a lot that still applies when you‚Äôre a smaller team trying to build with longevity and clarity in mind.\n\nThis post builds on a recent LinkedIn #BookmarkDive reflection, feel free to join the conversation there."
  },
  {
    "objectID": "blogs/unlearning.html",
    "href": "blogs/unlearning.html",
    "title": "üìå 10 Things to Unlearn to Truly Grow in Your Career",
    "section": "",
    "text": "I bookmarked this Fairygodboss article weeks ago, and coming back to it, I was struck by how clearly it speaks to the quieter, internal work of growth, the mindset shifts that don‚Äôt always get airtime but make all the difference.\nIt surfaces beliefs we often absorb without realizing: that we have to earn our seat at the table by being perfect, or that being early-career means being silent. One point in particular really stuck: learning to take up space and speak openly about what you want.\nüîó Read: 10 Things to Unlearn to Truly Grow in Your Career"
  },
  {
    "objectID": "blogs/unlearning.html#reflections",
    "href": "blogs/unlearning.html#reflections",
    "title": "üìå 10 Things to Unlearn to Truly Grow in Your Career",
    "section": "Reflections",
    "text": "Reflections\nThis idea echoes something I‚Äôve been thinking and writing about a lot lately: what it means to learn out loud. When we share our process, our questions, and even our uncertainty, we make it easier to move forward, and we signal to others that growth is a shared, ongoing practice.\nIt‚Äôs something I noticed in The Best Programmers I Know, too. The programmers and data analysts I admire most aren‚Äôt the ones who know everything, they‚Äôre the ones who keep learning in public, write things down, and create scaffolding for others. That‚Äôs the kind of professional I want to be.\nUnlearning perfectionism, self-effacement, and fear of being ‚Äútoo much‚Äù has changed how I show up in 1:1s, feedback sessions, peer reviews, and writing documentation. It‚Äôs helped me clarify my thinking, advocate for my growth, and contribute more meaningfully to the systems I work in.\nAnd that, I think, is what this piece gets so right: career growth doesn‚Äôt come from pretending to have it all figured out, rather it comes from making space to grow, out loud.\n\nThis post builds on a recent LinkedIn #BookmarkDive reflection, feel free to join the conversation there."
  },
  {
    "objectID": "blogs/awesome_python.html",
    "href": "blogs/awesome_python.html",
    "title": "üìå Awesome Python",
    "section": "",
    "text": "This is one of those classic GitHub resources I bookmarked a long time ago but only recently gave a proper look. I‚Äôve been in the process of refining which tools I actually want in my workflow, and this list has been a great companion for that.\nüìå Awesome Python"
  },
  {
    "objectID": "blogs/awesome_python.html#reflections",
    "href": "blogs/awesome_python.html#reflections",
    "title": "üìå Awesome Python",
    "section": "Reflections",
    "text": "Reflections\nWhat I appreciate about this list is how easy it makes it to scan the landscape without falling into a documentation spiral. When you‚Äôre trying to compare tools or see what‚Äôs out there beyond the defaults, it‚Äôs incredibly useful to have a high-level reference like this.\nWhy I keep coming back to it:\n\nIt‚Äôs organized by category, so you can zero in on what you‚Äôre actually trying to do (APIs, file handling, dashboards, testing, etc.). I think this helps a bit with the ‚Äúdrinking from a firehose feeling‚Äù.\nIt surfaces lesser-known but powerful libraries that don‚Äôt always show up in Google or StackOverflow searches.\nIt‚Äôs a solid reminder of how wide the Python ecosystem really is, and how much already exists, especially when you‚Äôre tempted to write something from scratch.\n\nI plan to start looking back at this more frequently as I clean up my own Python stack, deciding what‚Äôs core, what‚Äôs project-specific, and what‚Äôs just good to know exists when the right use case comes up.\n\nThis post builds on a recent LinkedIn #BookmarkDive reflection, feel free to join the conversation there."
  },
  {
    "objectID": "blogs/jenny_workflows.html",
    "href": "blogs/jenny_workflows.html",
    "title": "üìå Data Science Hangout with Jenny Bryan, Senior Software Engineer at Posit",
    "section": "",
    "text": "I recently attended a Data Science Hangout hosted by Posit PBC featuring Jenny Bryan, a familiar name for anyone who‚Äôs spent time in the R or Posit (formerly RStudio) communities.\nJenny‚Äôs writing and talks have shaped how I think about sustainable data work. It‚Äôs not just about making your code run, it‚Äôs about setting up systems that are understandable, reusable, and respectful of future-you (or future collaborators).\nHere are two resources created by Jenny that I revisited after the talk:\nüß∂ Project-oriented workflow\nüóÇÔ∏è Naming things"
  },
  {
    "objectID": "blogs/jenny_workflows.html#reflections",
    "href": "blogs/jenny_workflows.html#reflections",
    "title": "üìå Data Science Hangout with Jenny Bryan, Senior Software Engineer at Posit",
    "section": "Reflections",
    "text": "Reflections\nWhat I appreciate most about Jenny Bryan‚Äôs work is her commitment to showing her thinking. Whether it‚Äôs how she names files or organizes projects, her approach reinforces a broader message: the craft of programming isn‚Äôt just about code, it‚Äôs about how we work with each other.\nHearing her speak live during the Hangout reminded me how powerful it is to see someone model that ethos in real time. Jenny brings a grounded, candid energy to technical topics that could otherwise feel dry or overwhelming. She shared moments from her own transitions, from academia to software engineering, and it was validating to hear how much she gravitated toward the building and tinkering aspects of her work. I relate to that deeply.\nShe also talked about visibility and community with a humility that stood out. For someone so recognizable in the Posit ecosystem, she spoke with clarity about knowing who she is and the kind of work she wants to be doing. That‚Äôs the kind of career clarity I aspire to, and the kind of example I appreciate as I carve out my own path.\nThis is something I‚Äôve been reflecting on a lot lately, especially as I learn out loud and try to build systems that can grow with me. These resources reminded me that small decisions (like naming a file or setting up your folders) are not small at all. They shape your future ability to revisit, reuse, and explain your work.\nThey‚Äôre also part of how we show respect: for our teammates, for the community, and for ourselves.\n\nThis post builds on a recent LinkedIn #BookmarkDive reflection, feel free to join the conversation there."
  },
  {
    "objectID": "blogs/data_rescue.html",
    "href": "blogs/data_rescue.html",
    "title": "üìå The Data Rescue Project",
    "section": "",
    "text": "There‚Äôs something quietly powerful about the Data Rescue Project, a cross-institutional initiative to preserve vulnerable government datasets, especially environmental and climate data.\nI found this resource page a few months back and returned to it again recently. It‚Äôs a mix of tools, guides, and write-ups that center the idea of ‚Äúrescuing‚Äù public data before it disappears, either from neglect or intentional suppression. The whole project reminds me that data stewardship isn‚Äôt just about pipelines and dashboards. It‚Äôs about memory. And resistance.\nIf you‚Äôve ever wondered:\nThis resource library is a thoughtful entry point.\nüîó Explore the Data Rescue Resources"
  },
  {
    "objectID": "blogs/data_rescue.html#reflections",
    "href": "blogs/data_rescue.html#reflections",
    "title": "üìå The Data Rescue Project",
    "section": "Reflections",
    "text": "Reflections\nAs someone who works in research data and builds systems for traceability and transparency, I think a lot about the ethics of data lifecycle management. This project reinforced for me that open science requires open infrastructure, and that includes rescue, reproducibility, and resilience.\nEven outside moments of crisis, the skills and practices behind data rescue (i.e.¬†documentation, versioning, decentralized access, and reproducible infrastructure) are foundational. We shouldn‚Äôt need suppression or instability to remind us why long-term thinking about data matters.\n\nThis post builds on a recent LinkedIn #BookmarkDive reflection, feel free to join the conversation there."
  },
  {
    "objectID": "blogs/portfolios.html",
    "href": "blogs/portfolios.html",
    "title": "üìå How to Build a Data Analyst Portfolio: Tips for Success",
    "section": "",
    "text": "I saved this article as I started thinking more seriously about how to not just build skills, but actually show the work I‚Äôm doing. That doesn‚Äôt come naturally to me. I‚Äôve always been more comfortable behind the scenes: building systems, cleaning messy data, connecting dots. Not so much broadcasting the results.\nBut the more I grow in my career, the more I realize how important visibility is. Especially in data, where most of the work is invisible unless you choose to surface it.\nThis guide from Coursera is geared toward early-career analysts, but a few reminders really landed for me:\nüîó Read: How to Build a Data Analyst Portfolio"
  },
  {
    "objectID": "blogs/portfolios.html#reflections",
    "href": "blogs/portfolios.html#reflections",
    "title": "üìå How to Build a Data Analyst Portfolio: Tips for Success",
    "section": "Reflections",
    "text": "Reflections\nThis piece gave me permission to loosen up a bit, to treat my portfolio less like a high-stakes project and more like documentation. Not just for hiring managers, but for myself. A living space to track how I think, what I‚Äôm building, and how I‚Äôm evolving.\nI‚Äôm still figuring out what that looks like in practice. But reframing it as a form of self-documentation, not self-promotion, is helping. Im creating a quiet archive of growth, that might just be useful to someone else down the line too. Check out the Projects page of this site to see where I‚Äôm at so far in my journey to document my work experience and learning process.\nAnd I‚Äôm starting to shape these reflections into something more structured, a beginner-friendly guide called ‚ÄúZero to Quarto in 14 Days‚Äù for anyone who‚Äôs been stuck at ‚ÄúI should really make a portfolio‚Äù and wants to get one live on GitHub in just a couple weeks with some straightforward guides to help navigate the process. I‚Äôll share it on the Learn with Me page once it‚Äôs ready.\n\nThis post builds on a recent LinkedIn #BookmarkDive reflection, feel free to join the conversation there."
  },
  {
    "objectID": "blogs/clean_data.html",
    "href": "blogs/clean_data.html",
    "title": "üìå So you‚Äôve got a dataset, here‚Äôs how you clean it!",
    "section": "",
    "text": "Came across this article again while skimming through my old bookmarks. I remembered liking it the first time around for how grounded and practical it was ‚Äî and it still holds up.\nüìå So You‚Äôve Got a Dataset. Here‚Äôs How You Clean It."
  },
  {
    "objectID": "blogs/clean_data.html#reflections",
    "href": "blogs/clean_data.html#reflections",
    "title": "üìå So you‚Äôve got a dataset, here‚Äôs how you clean it!",
    "section": "Reflections",
    "text": "Reflections\nWhat I appreciate about this piece is that it‚Äôs not tied to a specific tool or language. You could apply the exact same process in R, Python, SQL, or Excel, the core ideas are all about intentional, structured thinking.\nSome solid reminders:\n\nStart by actually looking at your dataset. Dimensions, types, nulls, duplicates, don‚Äôt skip the orientation.\nDon‚Äôt treat missing data like an afterthought. Handle it with intention based on context.\nEncode categorical variables early and deliberately. It‚Äôll save you pain later.\nCheck your assumptions with basic summaries, sometimes the weirdness is obvious if you pause to look.\nModularize your cleaning steps. If you build them clearly, you can reuse them across projects.\n\nIt‚Äôs not fancy, but that‚Äôs the point. There‚Äôs something satisfying about a clean, thoughtful approach that isn‚Äôt trying to be clever, just effective. I‚Äôve found this kind of steady rhythm in cleaning always pays off, especially when I come back to a dataset weeks or months later and can still follow my own trail.\n\nThis post builds on a recent LinkedIn #BookmarkDive reflection, feel free to join the conversation there."
  },
  {
    "objectID": "blogs/r_pkg_dev.html",
    "href": "blogs/r_pkg_dev.html",
    "title": "üìå Awesome R Package Development Tools",
    "section": "",
    "text": "Every now and then I come across a resource I saved way back that still feels incredibly relevant, and this one definitely fits that category.\nüìå Awesome R Package Development Tools\n\nA curated collection of tools, workflows, and best practices for building R packages.\n\nI bookmarked this ages ago when I first started dreaming about turning our in-house MADC functions into a proper package. I still have a barebones repo sitting quietly in a corner of GitLab, mostly utility functions I use daily to pull and wrangle research data, but this site has stayed in the back of my mind as one of those ‚Äúwhen I‚Äôm ready to level this up‚Äù references.\nEven though I‚Äôm not actively building a package yet, revisiting this reminded me that there‚Äôs a whole ecosystem out there built to support that leap. It‚Äôs not just about having the right tools, it‚Äôs about seeing the path forward, even if you‚Äôre coming from an analyst background.\nWhat I like about this collection is that it doesn‚Äôt just list libraries, it actually walks you through how to think like a package developer, with guidance on structure, testing, documentation, and deployment.\nA few tools that stood out:\n\n{usethis} for scaffolding structure and setup, a lifesaver for project hygiene.\n{testthat} for building tests as I go (aspirational, but I‚Äôm trying).\n{pkgdown} for instantly making docs more shareable and browsable.\n{fledge} to automate changelogs and version bumps without the fuss.\nGitHub Actions templates for keeping tests and builds running smoothly behind the scenes.\n\nComing back to this list was a good reminder that if and when I do move forward with turning our internal scripts into something more formal and reusable, I won‚Äôt have to start from scratch. From scaffolding to polish to automation, it‚Äôs all here when I‚Äôm ready.\n\nThis post builds on a recent LinkedIn #BookmarkDive reflection, feel free to join the conversation there.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/git_stash.html",
    "href": "blogs/git_stash.html",
    "title": "üìå Git Stash for Newbies",
    "section": "",
    "text": "There‚Äôs a reason I keep coming back to Git Stash for Newbies, a beginner-friendly explainer written by Meghan Harris.\nIt‚Äôs short, approachable, and helped me finally wrap my head around a feature of Git that I‚Äôd heard about but never really integrated into my workflow: git stash.\nThe post walks through what stashing is, when you might use it, and how to avoid accidentally losing your work in the process. It‚Äôs a practical reminder that Git isn‚Äôt just for version control in the abstract, it‚Äôs also for workflow sanity when you‚Äôre context-switching, debugging, or just not ready to commit.\nüîó Read Git Stash for Newbies"
  },
  {
    "objectID": "blogs/git_stash.html#reflections",
    "href": "blogs/git_stash.html#reflections",
    "title": "üìå Git Stash for Newbies",
    "section": "Reflections",
    "text": "Reflections\nThis was one of those moments where a simple tool finally clicked because someone explained it in plain language. For someone like me, who often juggles work-in-progress code, quick bugfixes, and experiments, git stash has become a surprisingly helpful part of my toolkit.\nWhat struck me most is how this concept ties back to broader ideas in data and code management, how we hold space for unfinished work, how we reduce the cognitive burden of switching tasks, and how good tools support thoughtful pacing.\n\nThis post builds on a recent LinkedIn #BookmarkDive reflection, feel free to join the conversation there."
  },
  {
    "objectID": "blogs/deepwiki.html",
    "href": "blogs/deepwiki.html",
    "title": "üìå Deepwiki",
    "section": "",
    "text": "Not totally sure where I first stumbled across this one, but it‚Äôs a tool I‚Äôve bookmarked and revisited more than once, especially as I‚Äôve been cleaning up and refining some of my own repos.\nüìå Deepwiki"
  },
  {
    "objectID": "blogs/deepwiki.html#reflections",
    "href": "blogs/deepwiki.html#reflections",
    "title": "üìå Deepwiki",
    "section": "Reflections",
    "text": "Reflections\nWhat stood out to me about Deepwiki is how much context it manages to surface from a repo without you having to dig through every file. I‚Äôve run it on a few of my own personal projects recently and was genuinely impressed by how clearly it summarized the intent, layout, and dependencies in a way that made me rethink how I structure READMEs.\nIt‚Äôs helpful for:\n\nAuditing your own repos, making sure structure, naming, and purpose are legible to others.\nLearning from unfamiliar codebases, seeing how other tools are organized before diving into the weeds.\nJust exploring: their homepage is full of commonly-used open-source projects (like VS Code) that are fun to browse through from a systems-thinking perspective.\n\nI‚Äôve started using Deepwiki as a bit of a litmus test: does my repo make sense at a glance? Would someone new to the project understand what they‚Äôre looking at? And if not, what small tweaks would improve it?\n\nThis post builds on a recent LinkedIn #BookmarkDive reflection, feel free to join the conversation there."
  },
  {
    "objectID": "blogs/internet_archive.html",
    "href": "blogs/internet_archive.html",
    "title": "üìå Internet Archive: The Dataset Collection",
    "section": "",
    "text": "I came across the Internet Archive‚Äôs Datasets Collection again recently while cleaning out my bookmarks and was reminded why I saved it in the first place.\nIt‚Äôs a massive, searchable repository of open datasets, everything from historical records and media metadata to large-scale web scrapes and image dumps. Some are polished and well-documented. Others are raw, messy, and idiosyncratic. But taken together, they offer a uniquely real-world glimpse into the kinds of data challenges you won‚Äôt always find in curated public repositories.\nA few ways I imagine (or have already started) putting this to use:\nEven if you don‚Äôt need anything from it today, it‚Äôs a good one to keep in your back pocket. Or just explore. Some of the uploads are unexpected, oddly delightful, or historically fascinating."
  },
  {
    "objectID": "blogs/internet_archive.html#reflections",
    "href": "blogs/internet_archive.html#reflections",
    "title": "üìå Internet Archive: The Dataset Collection",
    "section": "Reflections",
    "text": "Reflections\nThis one doesn‚Äôt spark deep technical reflection so much as it grounds me in the bigger ecosystem of open data work. Not everything worth learning from comes from Kaggle or a polished API.\nThere‚Äôs value in getting familiar with weird formats. In downloading something that isn‚Äôt perfectly documented. In practicing the kind of judgment and troubleshooting that real-world work requires.\nIt‚Äôs a good reminder that open data isn‚Äôt always clean, and that‚Äôs where some of the best learning happens.\n\nThis post builds on a recent LinkedIn #BookmarkDive reflection, feel free to join the conversation there."
  },
  {
    "objectID": "blogs/awesome_quarto.html",
    "href": "blogs/awesome_quarto.html",
    "title": "üìå Awesome Quarto",
    "section": "",
    "text": "Now that I‚Äôve been using Quarto more regularly, both for my own portfolio and for my team‚Äôs open science site, I wanted to come back to this resource I saved early on but never really dug into until now.\nüìå Awesome Quarto"
  },
  {
    "objectID": "blogs/awesome_quarto.html#reflections",
    "href": "blogs/awesome_quarto.html#reflections",
    "title": "üìå Awesome Quarto",
    "section": "Reflections",
    "text": "Reflections\nWhen I first bookmarked this, Quarto still felt like a shiny new toy I hadn‚Äôt quite made space for. I skimmed it, bookmarked it, and figured I‚Äôd circle back eventually. Now that it‚Äôs become part of my real workflow, especially for publishing documentation and building out multi-page sites, I‚Äôm seeing this collection in a whole new light.\nThere‚Äôs a lot here, but a few areas I‚Äôm especially interested in as I keep building:\n\nMulti-page layouts that feel clean and navigable (especially for guides and internal documentation)\nPublishing workflows, what people are automating, and how they‚Äôre doing it\nEmbedded visualizations and examples that go beyond static charts\nHow folks are structuring content for collaboration and reuse, something I‚Äôm thinking a lot about as I try to make my scripts and documents more legible to teammates\n\nI‚Äôve been slowly building up my own library of snippets, templates, and patterns, and this repo is a good reminder that I don‚Äôt have to start from scratch, there‚Äôs already a growing community of folks figuring this out together.\n\nThis post builds on a recent LinkedIn #BookmarkDive reflection, feel free to join the conversation there."
  },
  {
    "objectID": "blogs/r_workflow.html",
    "href": "blogs/r_workflow.html",
    "title": "üìå R Workflow",
    "section": "",
    "text": "I always love stumbling across a bookmark I completely forgot about, the kind that makes you feel oddly grateful to past-you for spotting something useful before you even knew you‚Äôd need it.\nüìå R Workflow by Frank E Harrell Jr"
  },
  {
    "objectID": "blogs/r_workflow.html#reflections",
    "href": "blogs/r_workflow.html#reflections",
    "title": "üìå R Workflow",
    "section": "Reflections",
    "text": "Reflections\nThis isn‚Äôt a how-to or tutorial, it‚Äôs a focused, opinionated take on how to think about analytical projects in R, especially when working in collaborative or research-driven contexts. What I appreciated most was how it made me pause and reevaluate some of the defaults I‚Äôve accumulated over time.\nA few things that stood out:\n\nScript-first workflows: no hidden logic in interactive consoles or notebooks. If it matters, it should be in the script.\nProject structure is part of reproducibility, not an afterthought. Folder organization, file naming, and code modularity all impact how easily others (and future-you) can navigate the work.\nCommunication isn‚Äôt extra: comments, documentation, audit trails, they are the work. Especially in long-term or handoff-heavy settings.\n\nI‚Äôm not following this workflow to the letter (yet), but it resonated with some practices I‚Äôve already started to build around logging, commenting, and structuring code for clarity. It gave me a few nudges to go further and a bit of validation that I‚Äôm heading in the right direction, slowly, but intentionally.\nDefinitely one I‚Äôll return to as I keep shaping my own standards for what ‚Äúwell-structured‚Äù means in practice.\n\nThis post builds on a recent LinkedIn #BookmarkDive reflection, feel free to join the conversation there."
  },
  {
    "objectID": "blogs/ai_guick_guide.html",
    "href": "blogs/ai_guick_guide.html",
    "title": "üìå Using AI Right Now: A Quick Guide",
    "section": "",
    "text": "We‚Äôre in this moment where everyone‚Äôs trying to figure out what AI means for their work. Some folks are exploring new tools, some are worried about what it might take away. Personally, I‚Äôve found AI to be an enhancement, a way to:\nThis guide by Ethan Mollick is one I bookmarked for its clarity and practicality. It walks through how to use AI in your day-to-day work right now without hype or fearmongering.\nüîó Read: Using AI Right Now ‚Äì A Quick Guide"
  },
  {
    "objectID": "blogs/ai_guick_guide.html#reflections",
    "href": "blogs/ai_guick_guide.html#reflections",
    "title": "üìå Using AI Right Now: A Quick Guide",
    "section": "Reflections",
    "text": "Reflections\nOne of the key takeaways from the article is that prompting isn‚Äôt a party trick, it‚Äôs a skill to develop. The way you frame a question or structure a task can completely change the output you get. That‚Äôs been especially true in my own work, where I use AI tools not just to save time, but to think more clearly, draft early versions of tricky documentation, or get unstuck in a coding problem.\nMollick‚Äôs breakdown of where AI doesn‚Äôt add value is also refreshing. It‚Äôs not a replacement for domain expertise, and it can‚Äôt fix a broken process. But when used thoughtfully, it can act like a teammate, a second brain to bounce ideas off of or prototype something before it‚Äôs fully formed.\nFor me, the goal isn‚Äôt to use AI for everything. It‚Äôs to use it well, where it makes sense, and where it supports my broader intentions: clarity, creativity, and sustainable systems.\n\nThis post builds on a recent LinkedIn #BookmarkDive reflection, feel free to join the conversation there."
  },
  {
    "objectID": "blogs/document_everything.html",
    "href": "blogs/document_everything.html",
    "title": "üìå How to Document Everything at Work (Without Being Asked)",
    "section": "",
    "text": "I‚Äôm circling back to this Fairygodboss post that I saved several weeks back because it captures something most guides miss. The Smart Way to Document Everything at Work is one of those rare resources that applies equally well whether you‚Äôre in a technical or non-technical role.\nIt breaks down the why, when, and how of workplace documentation in a way that‚Äôs clear and non-preachy. There‚Äôs guidance on choosing the right format, understanding your audience, and avoiding the trap of writing for the sake of writing. It also validates the emotional labor that can come with documenting things that ‚Äúshould already be obvious.‚Äù But the biggest factor for me is how much it can reinforce your ability to self-advocate and understand your worth.\nüîó Read ‚ÄúThe Smart Way to Document Everything at Work‚Äù"
  },
  {
    "objectID": "blogs/document_everything.html#reflections",
    "href": "blogs/document_everything.html#reflections",
    "title": "üìå How to Document Everything at Work (Without Being Asked)",
    "section": "Reflections",
    "text": "Reflections\nI think a lot about systems, how we build them, how we explain them, how we hand them off. Documentation is one of the most under-appreciated parts of that cycle. For me, it‚Äôs a chance to clarify my own thinking and build scaffolding for the people who come after me, and even future me when I return to a project months or years down the road.\nThis post reminded me that good documentation isn‚Äôt just about saving time. It‚Äôs about respect. It‚Äôs about reducing invisible friction. And sometimes, it‚Äôs about protecting the integrity of a system long after you‚Äôve stepped away.\n\nThis post builds on a recent LinkedIn #BookmarkDive reflection, feel free to join the conversation there."
  },
  {
    "objectID": "blogs/best_programmers.html",
    "href": "blogs/best_programmers.html",
    "title": "üìå The Best Programmers I Know",
    "section": "",
    "text": "I saved this essay by Matthias Endler because it shifts the focus away from flashy skills and toward the quieter traits that make truly exceptional engineers: mindset, humility, and thoughtful habits.\nIt touches on something I‚Äôve noticed in my own growth: the best technical people I‚Äôve worked with are rarely loud. They ask thoughtful questions. They learn in public. They look at unfamiliar systems not with ego, but with curiosity. And they write things down.\nüîó Read ‚ÄúThe Best Programmers I Know‚Äù"
  },
  {
    "objectID": "blogs/best_programmers.html#reflections",
    "href": "blogs/best_programmers.html#reflections",
    "title": "üìå The Best Programmers I Know",
    "section": "Reflections",
    "text": "Reflections\nThis post reminded me how often we confuse ‚Äúsmart‚Äù with fast, or ‚Äúexperienced‚Äù with unteachable. The folks I admire most aren‚Äôt just great coders, they‚Äôre rigorous thinkers, generous explainers, and quiet stewards of institutional knowledge.\nIt also validates the time I spend on things that might not ‚Äúlook‚Äù technical, like documentation, onboarding guides, or just drawing out a messy diagram to make sense of something complex. That work matters. It compounds.\n\nThis post builds on a LinkedIn #BookmarkDive reflection. You can join the conversation here."
  },
  {
    "objectID": "blogs/fair_project.html",
    "href": "blogs/fair_project.html",
    "title": "üìå Setting up a FAIR and reproducible project",
    "section": "",
    "text": "I came across this one through the excellent RDM Weekly newsletter curated by Crystal Lewis and bookmarked it right away. It‚Äôs a quick, practical read on how to structure a project for clarity, collaboration, and long-term usability, all core to good research data management.\nüìå Setting up a FAIR and reproducible project"
  },
  {
    "objectID": "blogs/fair_project.html#reflections",
    "href": "blogs/fair_project.html#reflections",
    "title": "üìå Setting up a FAIR and reproducible project",
    "section": "Reflections",
    "text": "Reflections\nWhat stood out to me was how well this guide balances structure with flexibility. It‚Äôs not about rigid templates, it‚Äôs about building habits that make your work legible to others (and to your future self). The suggestions are approachable, practical, and easy to adapt across tools and teams.\nA few highlights:\n\nClear, simple folder and file naming conventions that lower the barrier to entry.\nEmphasis on project-level READMEs and contributor guides to make intent and roles transparent.\nEncouragement to keep things tool-agnostic and lightweight when possible, use formats and workflows that reduce dependency headaches.\nA reminder that reproducibility isn‚Äôt just technical, it‚Äôs about documentation, clarity, and making your work usable by people who weren‚Äôt there when it was built.\n\nThis post aligns really closely with how I‚Äôve been thinking about data work in collaborative research environments, where multiple people might touch the same pipeline (sometimes months apart). It‚Äôs also nudged me to revisit my own project templates and fill in some of the gaps I‚Äôve been meaning to address.\nDefinitely one I‚Äôll return to as I keep refining what ‚Äúgood infrastructure‚Äù looks like in my own work.\n\nThis post builds on a recent LinkedIn #BookmarkDive reflection, feel free to join the conversation there."
  },
  {
    "objectID": "projects/chicken_soup.html",
    "href": "projects/chicken_soup.html",
    "title": "Chicken Soup Data Visualization",
    "section": "",
    "text": "I created a series of interactive visualizations using Python and Altair, hosted on a Streamlit page. The goal was to allow viewers to explore various chicken soup recipes, focusing on nutrition, flu-fighting nutrients, and the impact of ingredients like dairy and allergens. Throughout the project, I addressed data limitations and refined the learning objectives, incorporating additional datasets from Open Food Facts to enhance the visualizations."
  },
  {
    "objectID": "projects/chicken_soup.html#project-overview",
    "href": "projects/chicken_soup.html#project-overview",
    "title": "Chicken Soup Data Visualization",
    "section": "",
    "text": "I created a series of interactive visualizations using Python and Altair, hosted on a Streamlit page. The goal was to allow viewers to explore various chicken soup recipes, focusing on nutrition, flu-fighting nutrients, and the impact of ingredients like dairy and allergens. Throughout the project, I addressed data limitations and refined the learning objectives, incorporating additional datasets from Open Food Facts to enhance the visualizations."
  },
  {
    "objectID": "projects/chicken_soup.html#key-accomplishments",
    "href": "projects/chicken_soup.html#key-accomplishments",
    "title": "Chicken Soup Data Visualization",
    "section": "Key Accomplishments",
    "text": "Key Accomplishments\n\nCreated interactive visualizations using Python, Altair, and Streamlit, enabling viewers to explore the nutritional content and ingredients of chicken soup recipes.\nDeveloped a distribution chart of nutrient facts, allowing users to adjust intervals and compare fat and calorie content across recipes.\nDesigned a chart to visualize protein and calcium levels relative to dairy and allergen content, with interactive filtering options for dietary preferences.\nCreated a visualization summarizing flu-fighting nutrients, allowing viewers to interact with detailed tooltips and compare recipes based on health scores.\nBuilt a final chart enabling users to filter recipes by serving size, fat/calorie content, popularity, and dietary restrictions, facilitating informed recipe selection.\nAdapted to data limitations by revising learning objectives and incorporating additional datasets for a more comprehensive analysis.\n\n View the deployed site\n View the source code"
  },
  {
    "objectID": "projects/madrc_package.html",
    "href": "projects/madrc_package.html",
    "title": "madRc - R Package",
    "section": "",
    "text": "Project Overview\nThis project involves the development of an R package in collaboration with other members of the Data Management and Statistical Core at the Michigan Alzheimer‚Äôs Disease Center. The goal is to compile the various R functions that have been written for day-to-day analysis and data retrieval tasks into a cohesive and efficient package. The package aims to streamline repetitive tasks, making it easier for team members to conduct their work without having to re-write commonly used functions. Once complete, the package will ideally be deployed on GitHub for easy access, allowing team members and the broader research community to download and use it. Currently, the project is still a work in progress, with ongoing development, testing, and refinement of the included functions.\n\n\nKey Accomplishments\n\nR Package Development:\n\nCollaborating with the Data Core team at MADC to compile various R functions used for day-to-day analysis and data retrieval.\nOrganizing functions into a cohesive package to streamline repetitive tasks and improve efficiency in data analysis workflows.\n\nPackage Deployment Plans:\n\nPlanning to deploy the package on GitHub for easy access and distribution, allowing team members and the wider data analysis community to download and use the package.\n\nCurrent Status:\n\nThe package is still in development, with ongoing efforts to test and refine the functions for usability and performance.\n\nCollaboration:\n\nWorking closely with other team members to ensure the package meets the needs of all users and integrates smoothly into existing workflows.\n\n\nDue to the current early stage of this project, I do not yet have a source code link to share. Check back soon!\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/horror-ific_movie_rec_system.html",
    "href": "projects/horror-ific_movie_rec_system.html",
    "title": "Horror-ific Movie Recommender System",
    "section": "",
    "text": "This project, completed as a final capstone for an Information Retrieval course at the University of Michigan School of Information, aimed to build a specialized recommender system tailored to enthusiasts of the Horror and Thriller film genres, filling a niche gap in the vast world of streaming platforms. While general streaming services like Netflix, Hulu, and Amazon Prime offer recommendations across all genres, these platforms do not focus on specific genres, such as Horror or Thriller, which have dedicated fan bases. By narrowing the scope of recommendations to only these genres and their sub-genres, this system seeks to provide personalized movie suggestions that align with the tastes of Horror/Thriller fans.\nThe data used for the recommender system comes from the MovieLens ‚Äúml-25m‚Äù dataset, which contains 25,000,095 ratings across 62,423 movies from 162,541 unique users. For this project, a subset of 7,665,481 ratings from 162,117 users for 11,958 unique Horror and Thriller films was used. The dataset also includes some hybrid sub-genres, such as Comedic Horror and Dramatic Thriller."
  },
  {
    "objectID": "projects/horror-ific_movie_rec_system.html#project-overview",
    "href": "projects/horror-ific_movie_rec_system.html#project-overview",
    "title": "Horror-ific Movie Recommender System",
    "section": "",
    "text": "This project, completed as a final capstone for an Information Retrieval course at the University of Michigan School of Information, aimed to build a specialized recommender system tailored to enthusiasts of the Horror and Thriller film genres, filling a niche gap in the vast world of streaming platforms. While general streaming services like Netflix, Hulu, and Amazon Prime offer recommendations across all genres, these platforms do not focus on specific genres, such as Horror or Thriller, which have dedicated fan bases. By narrowing the scope of recommendations to only these genres and their sub-genres, this system seeks to provide personalized movie suggestions that align with the tastes of Horror/Thriller fans.\nThe data used for the recommender system comes from the MovieLens ‚Äúml-25m‚Äù dataset, which contains 25,000,095 ratings across 62,423 movies from 162,541 unique users. For this project, a subset of 7,665,481 ratings from 162,117 users for 11,958 unique Horror and Thriller films was used. The dataset also includes some hybrid sub-genres, such as Comedic Horror and Dramatic Thriller."
  },
  {
    "objectID": "projects/horror-ific_movie_rec_system.html#key-accomplishments",
    "href": "projects/horror-ific_movie_rec_system.html#key-accomplishments",
    "title": "Horror-ific Movie Recommender System",
    "section": "Key Accomplishments",
    "text": "Key Accomplishments\n\nDeveloped a recommender system focused exclusively on Horror and Thriller films, differentiating it from general movie recommendation platforms.\nLeveraged the ‚Äúml-25m‚Äù dataset, a widely recognized dataset for recommender system experiments.\nSuccessfully experimented with multiple collaborative filtering methods, including the SVD algorithm and Keras-based neural networks.\nAchieved satisfactory RMSE results for initial experimentation, demonstrating the viability of collaborative filtering for niche movie recommendations.\nIdentified areas for future improvement, including fine-tuning the model to enhance personalization and exploring hybrid recommendation systems.\n\n View the final report"
  },
  {
    "objectID": "projects/fluffyfeelingco.html",
    "href": "projects/fluffyfeelingco.html",
    "title": "Fluffy Feeling Co.¬†Etsy Shop",
    "section": "",
    "text": "Fluffy Feeling Co. is an online business focused on creating and selling animal-themed gifts with a humorous, quirky, and lighthearted touch. The brand aims to provide adorable, relatable, and fun items that resonate with animal lovers. Products are designed to bring joy and smiles to customers, incorporating playful designs like dog-themed bandanas and print-on-demand gifts. Currently, Fluffy Feeling Co.¬†operates on platforms like Etsy, leveraging print-on-demand services via Printify for efficient production and fulfillment."
  },
  {
    "objectID": "projects/fluffyfeelingco.html#project-overview",
    "href": "projects/fluffyfeelingco.html#project-overview",
    "title": "Fluffy Feeling Co.¬†Etsy Shop",
    "section": "",
    "text": "Fluffy Feeling Co. is an online business focused on creating and selling animal-themed gifts with a humorous, quirky, and lighthearted touch. The brand aims to provide adorable, relatable, and fun items that resonate with animal lovers. Products are designed to bring joy and smiles to customers, incorporating playful designs like dog-themed bandanas and print-on-demand gifts. Currently, Fluffy Feeling Co.¬†operates on platforms like Etsy, leveraging print-on-demand services via Printify for efficient production and fulfillment."
  },
  {
    "objectID": "projects/fluffyfeelingco.html#key-accomplishments",
    "href": "projects/fluffyfeelingco.html#key-accomplishments",
    "title": "Fluffy Feeling Co.¬†Etsy Shop",
    "section": "Key Accomplishments:",
    "text": "Key Accomplishments:\n\nBrand Creation & Positioning:\n\nDeveloped a unique brand identity that resonates with animal lovers and those looking for humorous and quirky gift items.\nDefined core brand attributes: Happy, Funny, Quirky, Lighthearted, Adorable, and Relatable, to consistently shape all product offerings and marketing efforts.\n\nProduct Development & Design:\n\nCreated a wide range of products for various holidays and themes, including custom-designed items like dog bandanas, apparel, and home decor. These designs incorporate seasonal motifs, playful phrases, and unique styles that appeal to different customer segments.\n\nEtsy Shop Setup & Growth:\n\nLaunched Fluffy Feeling Co.¬†on Etsy, building a platform for product sales, customer engagement, and brand development.\nSuccessfully integrated a variety of products, leveraging print-on-demand services to manage production and inventory efficiently.\n\nSales & Marketing Strategy:\n\nCrafted seasonal and holiday-focused product campaigns, aligning product releases with key holidays like Valentine‚Äôs Day, Christmas, and Super Bowl Sunday, to maximize sales potential and customer interest.\nDeveloped a seasonal product timeline, detailing key events and strategies for product brainstorming, finalization, and marketing to ensure year-round sales momentum.\n\nCustomer Engagement & Feedback:\n\nFocused on building a community around Fluffy Feeling Co.¬†through social media and customer engagement, with the goal of growing the brand‚Äôs loyal customer base and understanding their preferences for future product offerings."
  },
  {
    "objectID": "projects/madc_study_suggester.html",
    "href": "projects/madc_study_suggester.html",
    "title": "Michigan Alzheimer‚Äôs Disease Center Study Suggester",
    "section": "",
    "text": "Project Overview\nThe MADC Study Suggester is an R script developed for the Michigan Alzheimer‚Äôs Disease Center (MADC) to help match participants with appropriate studies based on their specific criteria. The script gathers participant and study data, filters the available studies, and generates a list of recommended studies for each participant. The aim of this tool is to streamline the process of study selection, ensuring that participants are matched with studies that are relevant to their health status and research goals. The output of the script is a user-friendly, formatted PDF report that displays the study recommendations in a clear table. The script uses gt for creating well-structured tables, webshot2 for capturing snapshots, and pagedown for converting the results into a high-quality, printable PDF format.\n\n\nKey Accomplishments\n\nR Script Development:\n\nDeveloped an R script that gathers and filters participant and study data to generate study recommendations tailored to each participant‚Äôs specific criteria.\n\nOutput Generation:\n\nUsed gt to create structured, user-friendly tables and pagedown to convert the tables into high-quality, printable PDF reports.\n\nTools and Libraries:\n\nIntegrated webshot2 for snapshot generation and pagedown for rendering the final output as a PDF, enhancing the accessibility and presentation of the data.\n\nStreamlining Study Selection:\n\nThe tool improves the process of identifying appropriate studies for participants, saving time for researchers and ensuring better matching between participants and studies.\n\nCurrent Status:\n\nThe script is operational, providing formatted PDF outputs that are ready for use by MADC team members.\n\n\nDue to HIPAA regulations, I am not able to share the source code or sample output files.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/PEARL.html",
    "href": "projects/PEARL.html",
    "title": "Participant Engagement and Research Lifecycle (PEARL)",
    "section": "",
    "text": "This project, initiated on March 28, 2024, is an ongoing development with phased releases of features. The aim of the PEARL system (Participant Engagement and Research Lifecycle) is to improve participant visit tracking and engagement across 10+ research studies at the University of Michigan Alzheimer‚Äôs Disease Center.\nPEARL is designed as a network of interconnected lists, where each list represents a specific study. This network is intended to help participant and visit data flow seamlessly across co-enrolling studies, making it easier to manage participants who are enrolled in multiple studies simultaneously. By developing a modular workflow for each study, PEARL allows for customization to meet the needs of individual research projects while maintaining an integrated, scalable system.\nTo date, we have developed and implemented a list for the University of Michigan Memory and Aging Project (UMMAP) study, with plans to scale this system across all studies at the center. Power Automate is being used to connect data across these lists, ensuring smooth communication and data synchronization between the studies. Automated notifications are also incorporated to streamline task management and participant tracking."
  },
  {
    "objectID": "projects/PEARL.html#project-overview",
    "href": "projects/PEARL.html#project-overview",
    "title": "Participant Engagement and Research Lifecycle (PEARL)",
    "section": "",
    "text": "This project, initiated on March 28, 2024, is an ongoing development with phased releases of features. The aim of the PEARL system (Participant Engagement and Research Lifecycle) is to improve participant visit tracking and engagement across 10+ research studies at the University of Michigan Alzheimer‚Äôs Disease Center.\nPEARL is designed as a network of interconnected lists, where each list represents a specific study. This network is intended to help participant and visit data flow seamlessly across co-enrolling studies, making it easier to manage participants who are enrolled in multiple studies simultaneously. By developing a modular workflow for each study, PEARL allows for customization to meet the needs of individual research projects while maintaining an integrated, scalable system.\nTo date, we have developed and implemented a list for the University of Michigan Memory and Aging Project (UMMAP) study, with plans to scale this system across all studies at the center. Power Automate is being used to connect data across these lists, ensuring smooth communication and data synchronization between the studies. Automated notifications are also incorporated to streamline task management and participant tracking."
  },
  {
    "objectID": "projects/PEARL.html#key-achievements",
    "href": "projects/PEARL.html#key-achievements",
    "title": "Participant Engagement and Research Lifecycle (PEARL)",
    "section": "Key Achievements",
    "text": "Key Achievements\n\nNetwork of Interconnected Lists: Created a scalable framework that allows participant and visit data to flow seamlessly across studies, supporting co-enrollment and improving participant tracking across multiple research studies.\nModular Workflows for Study Customization: Developed customizable workflows for the UMMAP study and other studies, allowing each to adapt the system to their unique needs while maintaining consistency in participant management.\nPower Automate Integration: Integrated Power Automate to connect lists across studies, streamlining data flow and automating notifications to improve task management and participant engagement tracking.\nParticipant Engagement Metrics: Enhanced participant engagement tracking by integrating more robust data collection and reporting features, enabling better insights into participant behavior and interaction with research activities.\nCompliance with HIPAA: Ensured that all participant data is handled securely and in compliance with HIPAA regulations, prioritizing data privacy and security throughout the system‚Äôs development.\n\nDue to HIPAA regulations, I am not able to share examples of the PEARL system in action."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "This page showcases a diverse collection of projects spanning my professional work at the Michigan Alzheimer‚Äôs Disease Center (MADC), academic projects from my Master‚Äôs in Information (MSI) degree, and entrepreneurial ventures during my personal time. Each project reflects my skills in data analysis, programming, and strategic problem-solving, as well as my passion for creativity and innovation. Explore how I‚Äôve applied my expertise across different contexts to make an impact, from advancing research in healthcare to building my own brands.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Project\n        \n     \n  \n\n\n\n\n\n\n\nChicken Soup Data Visualization\n\n\n\ndata visualization\n\n\nstreamlit\n\n\npython\n\n\naltair\n\n\npandas\n\n\ngit\n\n\n\n\nApr 7, 2023\n\n\n\n\n\n\n\n\n\n\n\nCollaborative Filtering Methodologies\n\n\n\ncollaborative filtering\n\n\npython\n\n\nsurprise\n\n\nrecmetrics\n\n\npandas\n\n\ninformation retrieval\n\n\ndata mining\n\n\n\n\nDec 16, 2022\n\n\n\n\n\n\n\n\n\n\n\nData Analysis Capstone: Is Education the Key to Success?\n\n\n\nstreamlit\n\n\ndata visualization\n\n\npython\n\n\naltair\n\n\ngithub\n\n\nteam collaboration\n\n\nproject management\n\n\n\n\nApr 18, 2023\n\n\n\n\n\n\n\n\n\n\n\nData Management and Statistical Core Sharing Hub\n\n\n\nquarto\n\n\nmarkdown\n\n\ncss\n\n\ndata visualization\n\n\ndocumentation\n\n\nyaml\n\n\ngithub pages\n\n\n\n\nDec 12, 2024\n\n\n\n\n\n\n\n\n\n\n\nFluffy Feeling Co.¬†Etsy Shop\n\n\n\netsy\n\n\nentrepreneurship\n\n\nproduct design\n\n\nprint on demand\n\n\ncanva\n\n\naccounting\n\n\n\n\nApr 3, 2024\n\n\n\n\n\n\n\n\n\n\n\nHorror-ific Movie Recommender System\n\n\n\npython\n\n\nkeras\n\n\nsurprise\n\n\ninformation retrieval\n\n\ncollaborative filtering\n\n\nsvd\n\n\n\n\nDec 16, 2022\n\n\n\n\n\n\n\n\n\n\n\nMichigan Alzheimer‚Äôs Disease Center Resource Library\n\n\n\nquarto\n\n\nquarto pub\n\n\nr\n\n\nmarkdown\n\n\ngitlab\n\n\ncontinuous integration\n\n\nyaml\n\n\n\n\nNov 3, 2023\n\n\n\n\n\n\n\n\n\n\n\nMichigan Alzheimer‚Äôs Disease Center Study Suggester\n\n\n\nr\n\n\ntidyverse\n\n\ngt\n\n\nwebshot\n\n\npagedown\n\n\napi\n\n\n\n\nMay 13, 2024\n\n\n\n\n\n\n\n\n\n\n\nParticipant Engagement and Research Lifecycle (PEARL)\n\n\n\ndata management\n\n\nsystem design & development\n\n\nmicrosoft sharepoint\n\n\nmicrosoft power automate\n\n\nprocess diagramming\n\n\nteam collaboration\n\n\nhealthcare & research\n\n\nknowledge management\n\n\nworkflow automation\n\n\nproject management\n\n\ndata migration\n\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping for SMPL Sheets, LLC\n\n\n\nweb scraping\n\n\npython\n\n\nrequests\n\n\nbeautiful soup\n\n\nentrepreneurship\n\n\nmarketing\n\n\nhtml5\n\n\naccounting\n\n\nbusiness development\n\n\n\n\nNov 17, 2024\n\n\n\n\n\n\n\n\n\n\n\nWomen‚Äôs Web Design Project\n\n\n\nhtml5\n\n\ncss\n\n\njavascript\n\n\nweb accessibility\n\n\nweb design\n\n\ngithub pages\n\n\n\n\nApr 4, 2023\n\n\n\n\n\n\n\n\n\n\n\nmadRc - R Package\n\n\n\nr\n\n\nr package\n\n\ntechnical documentation\n\n\ntidyverse\n\n\napi\n\n\n\n\nOct 16, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "bookmarkdive.html",
    "href": "bookmarkdive.html",
    "title": " The BookmarkDive blog",
    "section": "",
    "text": "The BookmarkDive blog\nA curated trail of the resources, tools, and reflections shaping how I think about data, systems, and building things that last.\nEvery post starts with something I‚Äôve saved (a newsletter, a code snippet, a talk, a blog post) and ends with a short reflection on why it resonated, what I took away, or how I might use it. This series helps me stay grounded in my learning and make space for slow, intentional exploration, even when life and work move fast.\nSome entries are technical. Some are reflective. All of them mark a breadcrumb in my journey toward better data practice, deeper thinking, and more sustainable growth.\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Entry\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nEntry\n\n\nDate\n\n\n\n\n\n\nüìå R Workflow\n\n\n¬†\n\n\n\n\nüìå Awesome R Package Development Tools\n\n\nSep 1, 2025\n\n\n\n\nüìå Setting up a FAIR and reproducible project\n\n\nAug 29, 2025\n\n\n\n\nüìå Awesome Python\n\n\nAug 25, 2025\n\n\n\n\nüìå 9 Trends Shaping the Future of Data Management in 2025\n\n\nAug 22, 2025\n\n\n\n\nüìå So you‚Äôve got a dataset, here‚Äôs how you clean it!\n\n\nAug 18, 2025\n\n\n\n\nüìå Awesome Quarto\n\n\nAug 14, 2025\n\n\n\n\nüìå Deepwiki\n\n\nAug 11, 2025\n\n\n\n\nüìå Internet Archive: The Dataset Collection\n\n\nAug 8, 2025\n\n\n\n\nüìå How to Build a Data Analyst Portfolio: Tips for Success\n\n\nAug 1, 2025\n\n\n\n\nüìå Data Science Hangout with Jenny Bryan, Senior Software Engineer at Posit\n\n\nJul 31, 2025\n\n\n\n\nüìå Using AI Right Now: A Quick Guide\n\n\nJul 28, 2025\n\n\n\n\nüìå 10 Things to Unlearn to Truly Grow in Your Career\n\n\nJul 23, 2025\n\n\n\n\nüìå Git Stash for Newbies\n\n\nJul 16, 2025\n\n\n\n\nüìå The Data Rescue Project\n\n\nJul 9, 2025\n\n\n\n\nüìå How to Document Everything at Work (Without Being Asked)\n\n\nJul 2, 2025\n\n\n\n\nüìå The Best Programmers I Know\n\n\nJun 25, 2025\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]